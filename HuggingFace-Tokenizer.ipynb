{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Low-Level Hugging Face API: **Tokenizer**\n",
        "\n",
        "**Note**: It is recommended to use Google Colab"
      ],
      "metadata": {
        "id": "R35tiRoJ4tkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Setup"
      ],
      "metadata": {
        "id": "89zahMwK_Vpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "CTjLDWOI4tU8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4JOd2YJV4P9q"
      },
      "outputs": [],
      "source": [
        "# Login Hugging Face\n",
        "HUGGINGFACE_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "login(HUGGINGFACE_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models\n",
        "\n",
        "Each open-source model has its own rules, including tokenizer. We **must use their** (tokenizer) method when using their model.\n",
        "\n",
        "Do not use one model's token in another model because the token will be **meaningless**."
      ],
      "metadata": {
        "id": "n4sZ_rxWAnm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Models path\n",
        "llama_model = \"meta-llama/Llama-3.1-8B\"\n",
        "phi_model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "qwen_model = \"Qwen/Qwen2-7B-Instruct\"\n",
        "starcoder_model = \"bigcode/starcoder2-3b\" # Nvidia"
      ],
      "metadata": {
        "id": "y8Yt2RvMEvNM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Let's learn about tokenizer on LLM.\""
      ],
      "metadata": {
        "id": "JtDAz025FNUt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama3.1\n",
        "\n",
        "To use any Llama model, we first have to request access to Meta via the Hugging Face website. Then wait for them to accept our request.\n",
        "\n",
        "Source: https://huggingface.co/meta-llama/Llama-3.1-8B"
      ],
      "metadata": {
        "id": "oNLHHeEhBKGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=llama_model,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Encoder\n",
        "llama_tokens = llama_tokenizer.encode(text)\n",
        "print(llama_tokens)\n",
        "print(f\"length: {(len(llama_tokens))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRfENn5l_hxL",
        "outputId": "a971433c-7801-4ca3-d975-4012a2ccf0fc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[128000, 10267, 596, 4048, 922, 47058, 389, 445, 11237, 13]\n",
            "length: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder (Decoded tokens always have special tokens or indicators)\n",
        "llama_decoded = llama_tokenizer.decode(llama_tokens)\n",
        "llama_decoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9FjRgzUZApNC",
        "outputId": "d4f8a0ec-43d5-4dac-eca9-1d9100f82a07"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|begin_of_text|>Let's learn about tokenizer on LLM.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How the text is tokenized or encoded\n",
        "llama_batch_decoded = llama_tokenizer.batch_decode(llama_tokens)\n",
        "\n",
        "print(llama_batch_decoded)\n",
        "print(f\"length: {len(llama_batch_decoded)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqQYiXbLJN9G",
        "outputId": "347028ca-676a-4d3e-f423-7b3191306511"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<|begin_of_text|>', 'Let', \"'s\", ' learn', ' about', ' tokenizer', ' on', ' L', 'LM', '.']\n",
            "length: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model's vocab or dictionaries\n",
        "llama_vocab = llama_tokenizer.vocab\n",
        "print(f\"Llama vocab length: {len(llama_vocab)}\")\n",
        "\n",
        "# Gen model's special tokens\n",
        "llama_special_tokens = llama_tokenizer.get_added_vocab()\n",
        "print(f\"Llama special tokens length: {len(llama_special_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxdFM5S5LDZk",
        "outputId": "1cc98959-03e0-46a9-feaf-27046a401aab"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama vocab length: 128256\n",
            "Llama special tokens length: 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Llama Summary"
      ],
      "metadata": {
        "id": "qFQZlHWLKnib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original text\\t: {text}\")\n",
        "print(f\"Tokens\\t\\t: {llama_tokens} | (length:{len(llama_tokens)})\")\n",
        "print(f\"Batch decoded\\t: {llama_batch_decoded} | (length:{len(llama_batch_decoded)})\")\n",
        "print(f\"Decoded tokens\\t: {llama_decoded}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EBO2sOYHTXJ",
        "outputId": "102aea0c-6c74-46b5-e7da-9ad0c65e1fad"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text\t: Let's learn about tokenizer on LLM.\n",
            "Tokens\t\t: [128000, 10267, 596, 4048, 922, 47058, 389, 445, 11237, 13] | (length:10)\n",
            "Batch decoded\t: ['<|begin_of_text|>', 'Let', \"'s\", ' learn', ' about', ' tokenizer', ' on', ' L', 'LM', '.'] | (length:10)\n",
            "Decoded tokens\t: <|begin_of_text|>Let's learn about tokenizer on LLM.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phi\n",
        "\n",
        "Source: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"
      ],
      "metadata": {
        "id": "09Ob5QZdMkS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phi_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path=phi_model,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "phi_tokens = phi_tokenizer.encode(text)\n",
        "print(phi_tokens)\n",
        "print(f\"length: {len(phi_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI9RFis9Iegq",
        "outputId": "dbfac7b1-8fe4-4e1a-c31d-855b58d4041f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2803, 29915, 29879, 5110, 1048, 5993, 3950, 373, 365, 26369, 29889]\n",
            "length: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phi_batch_decoded = phi_tokenizer.batch_decode(phi_tokens)\n",
        "print(phi_batch_decoded)\n",
        "print(f\"length: {len(phi_batch_decoded)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXNk48PPM-s9",
        "outputId": "3e8b6866-b98c-4154-a2e1-8d4641823ede"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', \"'\", 's', 'learn', 'about', 'token', 'izer', 'on', 'L', 'LM', '.']\n",
            "length: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phi_decoded = phi_tokenizer.decode(phi_tokens)\n",
        "phi_decoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "egj8iJAuNjMO",
        "outputId": "a87dc666-5a66-4b18-83e5-3db8519229b4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Let's learn about tokenizer on LLM.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Phi vocab: {len(phi_tokenizer.vocab)}\")\n",
        "print(f\"Phi special tokens: {len(phi_tokenizer.get_added_vocab())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdhOWrWuOMeO",
        "outputId": "4aad61ed-0bb7-4b8f-b287-18d8b3e51048"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi vocab: 32011\n",
            "Phi special tokens: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qwen\n",
        "\n",
        "Source: https://huggingface.co/Qwen/Qwen2-7B-Instruct"
      ],
      "metadata": {
        "id": "bI0x3uU9Onfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    qwen_model,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "qwen_tokens = qwen_tokenizer.encode(text)\n",
        "print(qwen_tokens)\n",
        "print(f\"length: {len(qwen_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJFPuBiyOhu4",
        "outputId": "34c74767-c439-4cec-fd41-b5cd95e8bb63"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10061, 594, 3960, 911, 45958, 389, 444, 10994, 13]\n",
            "length: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_batch_decoded = qwen_tokenizer.batch_decode(qwen_tokens)\n",
        "\n",
        "print(qwen_batch_decoded)\n",
        "print(f\"length: {len(qwen_batch_decoded)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFSM0zo4PEk7",
        "outputId": "ba349e30-054d-4001-cf5b-9a28e44b5127"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Let', \"'s\", ' learn', ' about', ' tokenizer', ' on', ' L', 'LM', '.']\n",
            "length: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_decoded = qwen_tokenizer.decode(qwen_tokens)\n",
        "qwen_decoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UfeqaaDKPZ7T",
        "outputId": "be62c69a-c2ac-4389-8ab0-8625e6b82be1"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Let's learn about tokenizer on LLM.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Qwen vocab: {len(qwen_tokenizer.vocab)}\")\n",
        "print(f\"Qwen special tokens: {len(qwen_tokenizer.get_added_vocab())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw_p0XWdPi5s",
        "outputId": "6d4d3687-64ee-4147-90b7-4b3b4e948ee9"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qwen vocab: 151646\n",
            "Qwen special tokens: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StarCoder2\n",
        "\n",
        "Designed for program or code generation."
      ],
      "metadata": {
        "id": "N_yPL6eTUAxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "starcoder_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    starcoder_model,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "code=\"\"\"\n",
        "def greet(person):\n",
        "  print(f\"Hello, {person}!\")\n",
        "\"\"\"\n",
        "\n",
        "starcoder_tokens = starcoder_tokenizer.encode(code)\n",
        "\n",
        "print(starcoder_tokens)\n",
        "print(f\"length: {len(starcoder_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GmtzDgAUER1",
        "outputId": "7bc7689f-6389-42f0-b6e5-9c0c1248881f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[222, 610, 504, 7111, 45, 6427, 731, 353, 1489, 45, 107, 39, 8302, 49, 320, 6427, 130, 16013, 222]\n",
            "length: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "starcoder_batch_decoded = starcoder_tokenizer.batch_decode(starcoder_tokens)\n",
        "\n",
        "print(starcoder_batch_decoded)\n",
        "print(f\"length: {len(starcoder_batch_decoded)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9FeiuI5VFnZ",
        "outputId": "baac407f-8102-4a33-9542-24facdb9ffca"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', 'def', ' g', 'reet', '(', 'person', '):', '\\n ', ' print', '(', 'f', '\"', 'Hello', ',', ' {', 'person', '}', '!\")', '\\n']\n",
            "length: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in starcoder_tokens:\n",
        "  print(f\"{token} = {starcoder_tokenizer.decode(token)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g74qCY6TYcrx",
        "outputId": "4e84a898-8c63-42dd-bc82-dabee27cfb1a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "222 = \n",
            "\n",
            "610 = def\n",
            "504 =  g\n",
            "7111 = reet\n",
            "45 = (\n",
            "6427 = person\n",
            "731 = ):\n",
            "353 = \n",
            " \n",
            "1489 =  print\n",
            "45 = (\n",
            "107 = f\n",
            "39 = \"\n",
            "8302 = Hello\n",
            "49 = ,\n",
            "320 =  {\n",
            "6427 = person\n",
            "130 = }\n",
            "16013 = !\")\n",
            "222 = \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "starcoder_decoded = starcoder_tokenizer.decode(starcoder_tokens)\n",
        "starcoder_decoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uasujBOEVUbV",
        "outputId": "ed2e2cc7-91a7-49d5-c1bf-f8cf76277343"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef greet(person):\\n  print(f\"Hello, {person}!\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"StarCoder vocab: {len(starcoder_tokenizer.vocab)}\")\n",
        "print(f\"StarCoder special tokens: {len(starcoder_tokenizer.get_added_vocab())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bqRaLuyVt5C",
        "outputId": "4d7f1137-a916-43fc-f882-7c7f47250d26"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StarCoder vocab: 49152\n",
            "StarCoder special tokens: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Summary"
      ],
      "metadata": {
        "id": "zkEwJxJ0P9MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original text\\t: {text}\")\n",
        "\n",
        "print(f\"\\nLlama\")\n",
        "print(f\"Tokens\\t\\t: {llama_tokens}\")\n",
        "print(f\"Batch decoded\\t: {llama_batch_decoded}\")\n",
        "print(f\"Length\\t\\t: {len(llama_tokens)}\")\n",
        "\n",
        "print(f\"\\nPhi\")\n",
        "print(f\"Tokens\\t\\t: {phi_tokens}\")\n",
        "print(f\"Batch decoded\\t: {phi_batch_decoded}\")\n",
        "print(f\"Length\\t\\t: {len(phi_tokens)}\")\n",
        "\n",
        "print(f\"\\nQwen\")\n",
        "print(f\"Tokens\\t\\t: {qwen_tokens}\")\n",
        "print(f\"Batch decoded\\t: {qwen_batch_decoded}\")\n",
        "print(f\"Length\\t\\t: {len(qwen_tokens)}\")\n",
        "\n",
        "print(f\"\\n{50 * '='}\")\n",
        "\n",
        "print(f\"{code}\")\n",
        "\n",
        "print(\"\\nStarCoder\")\n",
        "print(f\"Tokens\\t\\t: {starcoder_tokens}\")\n",
        "print(f\"Batch decoded\\t: {starcoder_batch_decoded}\")\n",
        "print(f\"Length\\t\\t: {len(starcoder_tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FzhbgxxP6dc",
        "outputId": "7e95cb5c-d2f5-48a2-e3d8-fee044220624"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text\t: Let's learn about tokenizer on LLM.\n",
            "\n",
            "Llama\n",
            "Tokens\t\t: [128000, 10267, 596, 4048, 922, 47058, 389, 445, 11237, 13]\n",
            "Batch decoded\t: ['<|begin_of_text|>', 'Let', \"'s\", ' learn', ' about', ' tokenizer', ' on', ' L', 'LM', '.']\n",
            "Length\t\t: 10\n",
            "\n",
            "Phi\n",
            "Tokens\t\t: [2803, 29915, 29879, 5110, 1048, 5993, 3950, 373, 365, 26369, 29889]\n",
            "Batch decoded\t: ['Let', \"'\", 's', 'learn', 'about', 'token', 'izer', 'on', 'L', 'LM', '.']\n",
            "Length\t\t: 11\n",
            "\n",
            "Qwen\n",
            "Tokens\t\t: [10061, 594, 3960, 911, 45958, 389, 444, 10994, 13]\n",
            "Batch decoded\t: ['Let', \"'s\", ' learn', ' about', ' tokenizer', ' on', ' L', 'LM', '.']\n",
            "Length\t\t: 9\n",
            "\n",
            "==================================================\n",
            "\n",
            "def greet(person):\n",
            "  print(f\"Hello, {person}!\")\n",
            "\n",
            "\n",
            "StarCoder\n",
            "Tokens\t\t: [222, 610, 504, 7111, 45, 6427, 731, 353, 1489, 45, 107, 39, 8302, 49, 320, 6427, 130, 16013, 222]\n",
            "Batch decoded\t: ['\\n', 'def', ' g', 'reet', '(', 'person', '):', '\\n ', ' print', '(', 'f', '\"', 'Hello', ',', ' {', 'person', '}', '!\")', '\\n']\n",
            "Length\t\t: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuned Model for Chat (Model + Instruct)\n",
        "\n",
        "This model can understand chat formats like GPT (which has **system and user messages**)."
      ],
      "metadata": {
        "id": "CsRtFGsKQ-f9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model path\n",
        "chat_model = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "# Tokenizer\n",
        "chat_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    chat_model,\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "id": "yaGmAzKCQZOk"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an assistant who is good at making jokes\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a dad joke about machine learning.\"}\n",
        "]\n",
        "\n",
        "prompt = chat_tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False, # True: number tokens | False: text tokens\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "phi_prompt = phi_tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "qwen_prompt = qwen_tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "print(\"\\nLlama\")\n",
        "print(prompt)\n",
        "\n",
        "print(\"\\nPhi\")\n",
        "print(phi_prompt)\n",
        "\n",
        "print(\"\\nQwen\")\n",
        "print(qwen_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiZz54MlR9xZ",
        "outputId": "0869ab66-421a-492f-94e3-6886cce498c4"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Llama\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 26 Jul 2024\n",
            "\n",
            "You are an assistant who is good at making jokes<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Tell a dad joke about machine learning.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n",
            "\n",
            "Phi\n",
            "<|system|>\n",
            "You are an assistant who is good at making jokes<|end|>\n",
            "<|user|>\n",
            "Tell a dad joke about machine learning.<|end|>\n",
            "<|assistant|>\n",
            "\n",
            "\n",
            "Qwen\n",
            "<|im_start|>system\n",
            "You are an assistant who is good at making jokes<|im_end|>\n",
            "<|im_start|>user\n",
            "Tell a dad joke about machine learning.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_num = chat_tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True, # True: number tokens | False: text tokens\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "phi_prompt_num = phi_tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "qwen_prompt_num = qwen_tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "print(\"\\nLlama\")\n",
        "print(prompt_num)\n",
        "print(f\"length: {len(prompt_num)}\")\n",
        "\n",
        "print(\"\\nPhi\")\n",
        "print(phi_prompt_num)\n",
        "print(f\"length: {len(phi_prompt_num)}\")\n",
        "\n",
        "print(\"\\nQwen\")\n",
        "print(qwen_prompt_num)\n",
        "print(f\"length: {len(qwen_prompt_num)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M81i7uWJTLel",
        "outputId": "d1600d3a-bb6a-42be-86e4-edd8ffa6ecee"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Llama\n",
            "[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 2675, 527, 459, 18328, 889, 374, 1695, 520, 3339, 32520, 128009, 128006, 882, 128007, 271, 41551, 264, 18233, 22380, 922, 5780, 6975, 13, 128009, 128006, 78191, 128007, 271]\n",
            "length: 53\n",
            "\n",
            "Phi\n",
            "[32006, 887, 526, 385, 20255, 1058, 338, 1781, 472, 3907, 432, 23195, 32007, 32010, 24948, 263, 270, 328, 2958, 446, 1048, 4933, 6509, 29889, 32007, 32001]\n",
            "length: 26\n",
            "\n",
            "Qwen\n",
            "[151644, 8948, 198, 2610, 525, 458, 17847, 879, 374, 1661, 518, 3259, 31420, 151645, 198, 151644, 872, 198, 40451, 264, 17760, 21646, 911, 5662, 6832, 13, 151645, 198, 151644, 77091, 198]\n",
            "length: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-YMETz0-TnC-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}